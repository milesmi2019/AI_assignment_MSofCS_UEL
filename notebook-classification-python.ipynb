{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1099232,"sourceType":"datasetVersion","datasetId":614679}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom skimage.feature import hog\nfrom skimage.filters import sobel\nfrom skimage.morphology import binary_erosion, binary_dilation\nfrom skimage.exposure import histogram\n\n# Define the directory where the medical MNIST data resides on Kaggle\ndata_dir = '/kaggle/input/medical-mnist'\n# Define the target size for resizing images\nimage_size = 64\n# Define the names of the classes in our dataset\nclasses = ['AbdomenCT', 'BreastMRI', 'CXR', 'ChestCT', 'Hand', 'HeadCT']\n\n# Function to load the image data and corresponding labels from the directory\ndef load_data(data_directory):\n    images = []\n    labels = []\n    try:\n        # Iterate through each class directory\n        for class_id, class_name in enumerate(classes):\n            class_path = os.path.join(data_directory, class_name)\n            # Iterate through each image file in the class directory\n            for img_filename in os.listdir(class_path):\n                img_filepath = os.path.join(class_path, img_filename)\n                # Read the image in grayscale format\n                img = cv2.imread(img_filepath, cv2.IMREAD_GRAYSCALE)\n                # Check if the image was loaded successfully\n                if img is not None:\n                    # Resize the image to the defined target size (commented out initially)\n                    # img = cv2.resize(img, (image_size, image_size))\n                    # Ensure the image data type is unsigned 8-bit integer\n                    img = img.astype(np.uint8)\n                    images.append(img)\n                    labels.append(class_id)\n\n        # Print the shapes of the first 10 loaded images for inspection\n        print([img.shape for img in images[:10]])\n        # Print the unique shapes of all loaded images to check for consistency\n        print(np.unique([img.shape for img in images], axis=0))\n        return np.array(images), np.array(labels)\n    except FileNotFoundError:\n        print(f\"Error: Directory not found at {data_directory}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n    return np.array([]), np.array([])\n\n# Function to extract Histogram of Oriented Gradients (HOG) features from a set of images\ndef extract_hog_features(image_set):\n    hog_features = []\n    for img in image_set:\n        # Compute HOG features using predefined parameters\n        fd = hog(img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)\n        hog_features.append(fd)\n    return np.array(hog_features)\n\n# Function to extract edge features using the Sobel filter\ndef extract_edge_features(image_set):\n    edge_features = []\n    for img in image_set:\n        # Apply the Sobel filter to detect edges\n        edge_img = sobel(img)\n        # Flatten the edge image into a 1D array\n        edge_features.append(edge_img.flatten())\n    return np.array(edge_features)\n\n# Function to extract basic morphological features (erosion and dilation)\ndef extract_morphology_features(image_set):\n    morph_features = []\n    for img in image_set:\n        # Convert the grayscale image to binary using a threshold\n        binary_img = img > 127\n        # Apply binary erosion\n        eroded_img = binary_erosion(binary_img)\n        # Apply binary dilation\n        dilated_img = binary_dilation(binary_img)\n        # Concatenate the flattened eroded and dilated images\n        morph_feature = np.concatenate([eroded_img.flatten(), dilated_img.flatten()])\n        morph_features.append(morph_feature)\n    return np.array(morph_features)\n\n# Function to extract histogram features from the images\ndef extract_histogram_features(image_set):\n    hist_features = []\n    num_bins = 256\n    for img in image_set:\n        # Calculate the histogram of the image\n        hist, _ = histogram(img, nbins=num_bins, source_range='image')\n        # Pad or truncate the histogram to ensure a consistent length\n        if hist.shape[0] < num_bins:\n            padding = np.zeros(num_bins - hist.shape[0])\n            hist_padded = np.concatenate([hist, padding])\n            hist_features.append(hist_padded)\n        elif hist.shape[0] > num_bins:\n            hist_truncated = hist[:num_bins]\n            hist_features.append(hist_truncated)\n        else:\n            hist_features.append(hist)\n    return np.array(hist_features)\n\n# Function to extract all defined features and concatenate them\ndef extract_all_features(image_set):\n    hog_features = extract_hog_features(image_set)\n    edge_features = extract_edge_features(image_set)\n    morph_features = extract_morphology_features(image_set)\n    hist_features = extract_histogram_features(image_set)\n\n    expected_hog_len = 1764\n    expected_edge_len = image_size * image_size\n    expected_morph_len = 2 * image_size * image_size\n    expected_hist_len = 256\n\n    if hog_features.shape[1] != expected_hog_len:\n        print(f\"Warning: HOG feature length is {hog_features.shape[1]}, expected {expected_hog_len}\")\n    if edge_features.shape[1] != expected_edge_len:\n        print(f\"Warning: Edge feature length is {edge_features.shape[1]}, expected {expected_edge_len}\")\n    if morph_features.shape[1] != expected_morph_len:\n        print(f\"Warning: Morphology feature length is {morph_features.shape[1]}, expected {expected_morph_len}\")\n    if hist_features.shape[1] != expected_hist_len:\n        print(f\"Warning: Histogram feature length is {hist_features.shape[1]}, expected {expected_hist_len}\")\n\n    return np.hstack([hog_features, edge_features, morph_features, hist_features])\n\n# Function to train and evaluate a given classification model\ndef train_and_evaluate_model(model, X_train_data, y_train_labels, X_test_data, y_test_labels, model_name):\n    # Train the model on the training data\n    model.fit(X_train_data, y_train_labels)\n    # Make predictions on the test data\n    y_predicted = model.predict(X_test_data)\n    # Print the classification report\n    print(f\"{model_name} Classifier results:\")\n    print(classification_report(y_test_labels, y_predicted, target_names=classes))\n    # Calculate and print the accuracy of the model\n    accuracy = accuracy_score(y_test_labels, y_predicted)\n    print(f\"{model_name} accuracy rate: {accuracy}\")\n\n    # Visualize the confusion matrix\n    cm = confusion_matrix(y_test_labels, y_predicted)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n    plt.xlabel('Predicted label')\n    plt.ylabel('Original label')\n    plt.title(f'{model_name} confusion matrix')\n    plt.show()\n\n    return accuracy\n\n# Main execution block\nif __name__ == \"__main__\":\n    # Load the image data and labels\n    images, labels = load_data(data_dir)\n    # Check if the data loading was successful\n    if images.size == 0 or labels.size == 0:\n        print(\"Data loading failed. The program will now terminate.\")\n    else:\n        # Visualize the distribution of classes in the dataset\n        plt.figure(figsize=(10, 6))\n        sns.countplot(x=np.array(classes)[labels])\n        plt.xticks(rotation=45)\n        plt.xlabel('Category')\n        plt.ylabel('Sample Size')\n        plt.title('Distribution of Dataset Categories')\n        plt.show()\n\n        # Visualize a few sample images from the dataset\n        plt.figure(figsize=(12, 8))\n        for i in range(9):\n            plt.subplot(3, 3, i + 1)\n            plt.imshow(images[i], cmap='gray')\n            plt.title(f'Class: {classes[labels[i]]}')\n            plt.axis('off')\n        plt.show()\n\n        # Split the dataset into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n\n        # Print the shapes of the first 10 images and unique shapes in the training and testing sets\n        print(\"Shape of first 10 images in X_train:\", [img.shape for img in X_train[:10]])\n        print(\"Unique shapes in X_train:\", np.unique([img.shape for img in X_train], axis=0))\n        print(\"Shape of first 10 images in X_test:\", [img.shape for img in X_test[:10]])\n        print(\"Unique shapes in X_test:\", np.unique([img.shape for img in X_test], axis=0))\n\n        # Extract features from the training and testing sets\n        X_train_combined = extract_all_features(X_train)\n        X_test_combined = extract_all_features(X_test)\n\n        # Initialize and train a Linear Support Vector Machine (SVM) classifier\n        svm_classifier = LinearSVC(max_iter=10000)\n        svm_accuracy = train_and_evaluate_model(svm_classifier, X_train_combined, y_train, X_test_combined, y_test, 'SVM')\n\n        # Initialize and train a k-Nearest Neighbors (k-NN) classifier\n        knn_classifier = KNeighborsClassifier(n_neighbors=5)\n        knn_accuracy = train_and_evaluate_model(knn_classifier, X_train_combined, y_train, X_test_combined, y_test, 'k-NN')\n\n        # Visualize the comparison of model accuracies\n        models = ['SVM', 'k-NN']\n        accuracies = [svm_accuracy, knn_accuracy]\n        plt.figure(figsize=(8, 6))\n        sns.barplot(x=models, y=accuracies)\n        plt.xlabel('Models')\n        plt.ylabel('Accuracy')\n        plt.title('Comparison of Model Accuracies')\n        for i, v in enumerate(accuracies):\n            plt.text(i, v, f'{v:.4f}', ha='center')\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}